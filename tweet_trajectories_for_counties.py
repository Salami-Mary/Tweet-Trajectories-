# -*- coding: utf-8 -*-
"""Tweet Trajectories for counties.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cVIwDg2EfNwiNbjbVscVeuPTGJcJyK0n
"""

#DATA

import pandas as pd  #importamt libraries

SLO= pd.read_csv("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/SBB4.csv") #readomg the prefire data for santa luios county

SLO.head(5)

SLO.describe()

SLO.info() #understanding the data

slob4=SLO.dropna(axis=1, how='all')

slob4.info()

slob4.info()

#slob4.drop (["author.entities.description.cashtags"], axis=1)

slov=slob4.drop(["author.public_metrics.tweet_count","author.public_metrics.listed_count","author.public_metrics.tweet_count","author.public_metrics.followers_count", "author.profile_image_url","author.pinned_tweet_id","author.url","author.entities.url.urls","author.entities.description.mentions","author.entities.description.hashtags"], axis=1)

slov.info()

slo=slob4.drop(["__twarc.version","__twarc.retrieved_at","author.public_metrics.tweet_count","author.entities.description.urls","attachments.poll_ids","attachments.poll.voting_status","attachments.poll.options","attachments.poll.id","attachments.poll.end_datetime","attachments.poll.duration_minutes","attachments.media_keys","attachments.media","context_annotations"] ,axis=1)

slo.info()

slov=slob4.drop(["author.public_metrics.listed_count","author.public_metrics.following_count","author.public_metrics.followers_count","author.protected","author.profile_image_url","attachments.poll_ids","author.pinned_tweet_id","author.entities.url.urls","author.entities.description.mentions","author.entities.description.hashtags","entities.urls","entities.mentions","entities.hashtags","entities.annotations"],axis=1)

slov.info()

b=slov.drop(["edit_history_tweet_ids","attachments.poll.voting_status","edit_controls.edits_remaining","edit_controls.editable_until","edit_controls.is_edit_eligible","possibly_sensitive","context_annotations","reply_settings","__twarc.version","__twarc.url"],axis=1)

b.info()

c=b.drop(["public_metrics.impression_count","public_metrics.reply_count","public_metrics.quote_count","public_metrics.like_count","attachments.media","attachments.media_keys" ,"attachments.poll.duration_minutes"],axis=1)

c.info()

d=c.drop(["attachments.poll.end_datetime","attachments.poll.id","attachments.poll.options","author.entities.description.urls","author.public_metrics.tweet_count","author.verified","in_reply_to_username","in_reply_to_user_id"],axis=1)

d.info()

e=d.drop(["referenced_tweets.replied_to.id","referenced_tweets.quoted.id","quoted_username","lang","author.description","author.url","public_metrics.retweet_count"],axis=1)

e.info()

f=e.drop(["__twarc.retrieved_at","quoted_user_id","geo.country","geo.country_code","geo.full_name","geo.id","geo.coordinates.type","geo.geo.type","geo.name","geo.place_id","author.id","author.username","author.name","author.location","geo.place_type"],axis=1)

f.head()

f.info()

f.to_excel("SBB4.xlsx")

h= pd.read_excel("/content/SBB4.xlsx")

h.info()

cd

"""##SANTA BARBARA COUNTY PREPROCESSING

#BEFORE WILDFIRE
"""

import pandas as pd

SB= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SBB4.xlsx")

SB.head(5)

SB['X'] = SB['X'].fillna(SB.pop('X2'))

SB.head(5)

SB['Y'] = SB['Y'].fillna(SB.pop('Y2'))

SB.head(5)

SB4=SB.drop(["id","conversation_id"], axis=1)

SB4.head(5)

"""#CONTAINED WILDFIRE"""

import pandas as pd

SBCT= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SBCT.xlsx")

sbct=SBCT.drop(["entities.cashtags","author.created_at","id","conversation_id"], axis=1)

sbct['X'] = sbct['X'].fillna(sbct.pop('X2'))
sbct['Y'] = sbct['Y'].fillna(sbct.pop('Y2'))

sbct.head(5)

"""#After Wildfire """

sbaft= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SBAFTER.xlsx")

sbaft.head(5)

sbat=sbaft.drop(["id","conversation_id"], axis=1)

sbat.head(5)

sbat['X'] = sbat['X'].fillna(sbat.pop('X2'))
sbat['Y'] = sbat['Y'].fillna(sbat.pop('Y2'))

sbat.head(5)

"""#Ventura Start"""

VTST= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/VENTSTART.xlsx")

VTST.head(5)

vtst=VTST.drop(["id","conversation_id"], axis=1)
vtst['X'] =vtst['X'].fillna(vtst.pop('X2'))
vtst['Y'] =vtst['Y'].fillna(vtst.pop('Y2'))

"""#Ventura CT"""

VTCT= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/VENTCT.xlsx")

VTCT.head(5)

vtct=VTCT.drop(["id","conversation_id"], axis=1)

vtct['X'] =vtct['X'].fillna(vtct.pop('X2'))
vtct['Y'] =vtct['Y'].fillna(vtct.pop('Y2'))

vtct.head(5)

"""#Ventura After"""

vtat= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/VENTAFTER.xlsx")

vtat.head(5)

vtaf=vtat.drop(["id","conversation_id"], axis=1)

vtaf['X'] =vtaf['X'].fillna(vtaf.pop('X2'))
vtaf['Y'] =vtaf['Y'].fillna(vtaf.pop('Y2'))

vtaf.head(5)

"""#SANTA LUIS START"""

sls= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SAnta_LUIS.xlsx")

sls.head()

sls=sls.drop(["id","conversation_id"], axis=1)

sls['X'] =sls['X'].fillna(sls.pop('X2'))
sls['Y'] =sls['Y'].fillna(sls.pop('Y2'))

sls

"""#Santa Luis Ct"""

slst= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SAnta_LUISct.xlsx")

slst.head()

slst=slst.drop(["id","conversation_id"], axis=1)
slst['X'] =slst['X'].fillna(slst.pop('X2'))
slst['Y'] =slst['Y'].fillna(slst.pop('Y2'))

slst.head()

"""#Santa Luis AFTER"""

slat= pd.read_excel("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/CLEANED 2/SAnta_LUISafter.xlsx")

slat.head()

slat=slat.drop(["id","conversation_id"], axis=1)
slat['X'] =slat['X'].fillna(slat.pop('X2'))
slat['Y'] =slat['Y'].fillna(slat.pop('Y2'))

slat

"""#Interpolation of author ID """

list_post= list(SB4['author_id'])
list_cont=list(sbct['author_id'])
list_pre=list(sbat['author_id'])

def intersection(list_pre, list_post, list_cont):
                 list = [value for value in list_pre if value in list_post if value in list_cont]
                 return list

x=(intersection(list_post, list_pre, list_cont))

len(x)

filt_post= SB4['author_id'].isin(x)

SBpost=SB4.loc[filt_post]

SBpost.head(5)

len(SBpost)

filt_post= sbct['author_id'].isin(x)

SBct=sbct.loc[filt_post]

SBct.head(5)

len(SBct)

F=SBpost['author_id'].unique()

len(F)

k=SBct['author_id'].unique()

len(k)

filt_post= sbat['author_id'].isin(x)

sbaf=sbat.loc[filt_post]

sbaf.head(5)

j=sbaf['author_id'].unique()
len(j)

sbaf.columns

SBct.columns

SBpost.columns

SB_FIRE=pd.concat([sbaf,SBct,SBpost], axis=0)

SB_FIRE.head(5)

len(SB_FIRE)

SB_FIRE.to_csv("sb_fire.csv")

"""#VENTURA INTERPOLATION"""

vt_post= list(vtaf['author_id'])
vt_cont=list(vtct['author_id'])
vt_pre=list(vtst['author_id'])

def intersection(vt_pre, vt_post, vt_cont):
                 list = [value for value in vt_pre if value in vt_post if value in vt_cont]
                 return list

x=(intersection(vt_post, vt_pre, vt_cont))

len(x)

filt_vtpost= vtaf['author_id'].isin(x)
vtpost=vtaf.loc[filt_vtpost]
filt_vtct= vtct['author_id'].isin(x)
vtctt=vtct.loc[filt_vtct]
filt_vtstt= vtst['author_id'].isin(x)
vtpre=vtst.loc[filt_vtstt]

len(vtpre)

len(vtctt)

f=vtpre['author_id'].unique()
len(f)

VT_FIRE=pd.concat([vtctt,vtpre,vtpost], axis=0)
VT_FIRE.head(5)
len(VT_FIRE)

VT_FIRE.to_csv("vt_fire2.csv")

"""#Santa Louis Interpolatiom"""

sl_post= list(slat['author_id'])
sl_cont=list(sls['author_id'])
sl_pre=list(slst['author_id'])

x=(intersection(sl_post, sl_pre, sl_cont))

len(x)

filt_slpost= slat['author_id'].isin(x)
slpost=slat.loc[filt_slpost]
filt_slct= sls['author_id'].isin(x)
slctt=sls.loc[filt_slct]
filt_slst= slst['author_id'].isin(x)
slpre=slst.loc[filt_slst]

f=slctt['author_id'].unique()
len(f)

SL_FIRE=pd.concat([slpre,slctt,slpost], axis=0)
SL_FIRE.head(5)
len(SL_FIRE)

SL_FIRE.head(3)

cd /content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/interpolated

SL_FIRE.to_csv("sl_fire2.csv")

"""#MERGING ALL THREE DATAFRAMES """

sbfire=pd.read_csv("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/interpolated/sb_fire.csv")
slfire=pd.read_csv("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/interpolated/sl_fire2.csv")
vtfire=pd.read_csv("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/interpolated/vt_fire2.csv")

merged_df=pd.concat([sbfire,slfire,vtfire], axis=0)

merged_df.head(5)

merged_df.drop(['Unnamed: 0','Unnamed: 0.1','author.created_at'], axis =1)

merged_df["author_id"]

from datetime import datetime
import datetime as dt
merged_df['Date'] = pd.to_datetime(merged_df['created_at'], errors='coerce')

merged_df

merged_df['Date'] = merged_df['Date'].dt.strftime('%Y-%m-%d')

merged_df.head(5)

merged_df.drop(["Unnamed: 0","Unnamed: 0.1","author.created_at","created_at"],axis=1,inplace= True)

merged_df

#groupby author id .. all the points for all persons  average of x and average y 
#diivided by the number of locations each person visited
#convert to geo coordinate, meters

"""## Data Visualization"""

import seaborn
import datetime
import matplotlib.pyplot as plt

all=pd.read_csv("/content/drive/MyDrive/RESEARCH 2023/NEW FIRE DATASET/MERGED/allcounties2.csv")

#A box plot of tweet per person per day over time, highlighting the fire timeline

all['Date'].min(), all['Date'].max()

all['Dates'] = pd.to_datetime(all['Date'], errors='coerce')

all['Dates'] = all['Dates'].dt.strftime('%Y-%m-%d')

F= all.groupby(['Dates', 'author_id']).size()
F

df = F.to_frame().reset_index()
df = F.to_frame().reset_index()
df = df.rename(columns= {0: 'freq'})

df.index.name = 'index'

df.info()

df.head(4)

df['Dates']= pd.to_datetime(df['Dates'])
df['month_day'] = df['Dates'].dt.strftime('%m-%d')

df.tail(2)

#df.reset_index( inplace=True)

df['author_id'] = df['author_id'].astype(str)

"""##Daily Distribution per person"""

fig, ax = plt.subplots(figsize=(40,10))
seaborn.boxplot(x = df['month_day'], 
                y = df['freq'], 
                ax = ax,showfliers=False,
                color= 'white')
#ax.axvline(df.iloc[2000,0], color='deeppink')
 
                                                    
plt.xticks(rotation=90)                                                               
plt.tight_layout()                                                                    
#splot.yaxis.grid(True, clip_on=False)                                                 
#sns.despine(left=True, bottom=True)   

plt.xlabel('Date', fontsize=30);
plt.ylabel('Frequency', fontsize=30);
plt.title('Daily Distribution', fontsize=30)                                        
plt.tick_params(axis='both', which='major', labelsize=25)
plt.savefig('box')

"""##DAILY TWEETS"""

df.iloc[1000,0]

#Number of Tweets per day

all.head()

all['Dates'] = pd.to_datetime(all['Date'], format='%Y-%m-%d',errors='coerce')

df = all.set_index(['Date'])
df.head(2)

def f(x):
     return Series(dict(Number_of_tweets = x['text'].count(), 
                        ))

df.index = pd.to_datetime(df.index)

df

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas import DataFrame
from pandas import Series

daily_count = df.groupby(df.index.date).apply(f)

print (len(daily_count))
daily_count.head(10)

type(daily_count)

daily_count.index.name = 'date'
daily_count.head(5)

daily_count.index.min()

daily_count.index.max()

df.info()

from pandas.plotting import plot_params
fig, ax = plt.subplots(figsize=(24,10))
daily_count.Number_of_tweets.plot(kind='hist',edgecolor='black')
plt.title("Number of Tweets per day")
plt.xlabel('Number of Tweets')

daily_count.reset_index(inplace=True)

daily_count.head(5)

daily_count.dtypes

daily_count['dates'] = pd.to_datetime(daily_count['date'])
daily_count['month_day'] = daily_count['dates'].dt.strftime('%m-%d')

daily_count.head(5)

import seaborn as sns

sns.set_style('whitegrid')
fig,ax=plt.subplots(figsize=(24,8))
plt.plot(daily_count['month_day'], daily_count['Number_of_tweets'], color='red',linewidth=2)

ax.axvline(("11-25"), color='deeppink')
ax.axvline(("12-14"), color='green')
plt.xlabel('Datetime')
plt.ylabel('Number of Daily Tweets')
plt.title('Number of Daily Tweets over Time')
plt.xticks(rotation=90)
plt.show()

"""#A histogram of number of tweeter accounts per day"""

def f(x):
     return Series(dict(Number_of_tweets =len( x['author_id'].unique()), 
                        ))

df.index = pd.to_datetime(df.index)

acct_count = df.groupby(df.index.date).apply(f)

acct_count.head(5)

fig, ax = plt.subplots(figsize=(40,10))
acct_count.Number_of_tweets.plot(kind='hist',bins=8,edgecolor='black' )
plt.xlabel('Twitter accounts', fontsize=30);
plt.ylabel('Frequency', fontsize=30);
plt.title('Number of Twitter accounts per day', fontsize=30)                                        
plt.tick_params(axis='both', which='major', labelsize=25)

acct_count.index.name = 'date'

acct_count.reset_index(inplace=True)

acct_count['dates'] = pd.to_datetime(acct_count['date'])
acct_count['month_day'] = acct_count['dates'].dt.strftime('%m-%d')

sns.set_style('whitegrid')
fig,ax=plt.subplots(figsize=(24,8))
plt.plot(acct_count['month_day'], acct_count['Number_of_tweets'], color='red',linewidth=2)

ax.axvline(("11-25"), color='deeppink')
ax.axvline(("12-14"), color='green')

plt.xlabel('Datetime')
plt.ylabel('Number of accounts')
plt.title('Number of Twitter accounts over Time')
plt.xticks(rotation=90)

plt.show()

"""#Radius of gyration"""

#pip install utm

all.head(2)

#pip install scikit-mobility

import skmob
from skmob.measures.individual import radius_of_gyration

h=(all.author_id).unique()
len(h)

all.head(5)

allvalue=all.groupby('Dates')['author_id'].value_counts()
allvalues=allvalue[allvalue>3]
allv=allvalues.to_frame('counts')
type(allv)

allv.reset_index(inplace=True)

allv.head(5)

len((allv.author_id).unique())

allvv=list(allv['author_id'])
allj=list(all['author_id'])

def intersection(vt_pre, vtpost):
                 list = [value for value in vt_pre if value in vtpost]
                 return list

x=(intersection(allvv,allj))

filt_all= all['author_id'].isin(x)
slall=all.loc[filt_all]
filt_allj= allv['author_id'].isin(x)
slallj=allv.loc[filt_allj]

rgd_dist=pd.concat([slall,slallj], axis=0)
rgd_dist.head(5)

tdf = skmob.TrajDataFrame(all, latitude='Y', longitude='X', datetime='Dates', user_id='author_id')

rg_df = radius_of_gyration(tdf)

type(rg_df)

rg_df.head(6)